{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHYS243: Foundation of Applied Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>Final Project - Neural Networks</font><br>\n",
    "<font size=3>Prof. Bahram Mobasher, Inst. Abtin Shahidi</font><br>\n",
    "<font size=3>Submitted By:</font><br>\n",
    "<font size=3>&nbsp;&nbsp;&nbsp;Ray Felipe</font><br>\n",
    "<font size=3>&nbsp;&nbsp;&nbsp;Student ID: 862120029</font><br>\n",
    "<font size=3>&nbsp;&nbsp;&nbsp;Aug. 19, 2019</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this final project, we'll build a neural network to classify a test data set. A neural network is a network or circuit of neurons, or in a modern sense, an artificial neural network, composed of artificial neurons or nodes. Thus a neural network is either a biological neural network, made up of real biological neurons, or an artificial neural network, for solving artificial intelligence (AI) problems.[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Build a Neural Network Model\n",
    "#### As you saw in the lectures and notebook, neural nets are quite effective in classification if you build a reasonable network, and here you are going to build a neural network model for a very simple classification task on the data provided here. (Use any library you want but explain any method you use from them)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load and pre-process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before any data analysis project can begin, we must perform data loading and preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(685, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('dataset/train_set.txt', header=None)\n",
    "print(df.shape)\n",
    "df.columns=['col1', 'col2', 'col3_class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build X input for analysis. The function takes in the dataframe, our train data, along with input_num. As outlined in the exercise definition, the input_num is defined as follows:\n",
    "\n",
    "    1. {X3, X4}\n",
    "    2. {X3, X5}\n",
    "    3. {X3, X4, X5}\n",
    "    4. {X1, X2, X3, X4, X5}\n",
    "    0. default - original attribute in the dataset, X1 and X2\n",
    "\n",
    "    Formula:\n",
    "    Xsub3 = Xsub1^2\n",
    "    Xsub4 = Xsub2^2\n",
    "    Xsub5 = Xsub1 * Xsub2\n",
    "    \n",
    "<i>This addresses the question in Section 3.0 below - Building New Attributes.</i>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_X(dataframe_for_X, input_num=0):\n",
    "    \n",
    "    df = dataframe_for_X\n",
    "    X = []\n",
    "\n",
    "    if input_num == 0:\n",
    "        for i in range(len(df)):\n",
    "            X_pair = []\n",
    "            X_pair.append(df.col1[i])\n",
    "            X_pair.append(df.col2[i])\n",
    "            #X_pair.append(df.col2[i]*2) # new attributes to add\n",
    "            X.append(X_pair)\n",
    "        X_numpyarray = np.array([np.array(xi) for xi in X])\n",
    "        X = X_numpyarray\n",
    "\n",
    "    if input_num == 1:\n",
    "        for i in range(len(df)):\n",
    "            X_pair = []\n",
    "            X_pair.append(df.col1[i]**2) #X3 = X1**2\n",
    "            X_pair.append(df.col2[i]**2) #X4 = X2**2\n",
    "            X.append(X_pair)\n",
    "        X_numpyarray = np.array([np.array(xi) for xi in X])\n",
    "        X = X_numpyarray\n",
    "\n",
    "    if input_num == 2:\n",
    "        for i in range(len(df)):\n",
    "            X_pair = []\n",
    "            X_pair.append(df.col1[i]**2) #X3 is X1**2\n",
    "            X_pair.append(df.col1[i] * df.col2[i]) #X5 is X1 * X2\n",
    "            X.append(X_pair)\n",
    "        X_numpyarray = np.array([np.array(xi) for xi in X])\n",
    "        X = X_numpyarray\n",
    "\n",
    "    if input_num == 3:\n",
    "        for i in range(len(df)):\n",
    "            X_pair = []\n",
    "            X_pair.append(df.col1[i]**2) #X3 is X1**2\n",
    "            X_pair.append(df.col2[i]**2) #X4 is X1**2\n",
    "            X_pair.append(df.col1[i] * df.col2[i])  # X5 is X1 * X2\n",
    "            X.append(X_pair)\n",
    "        X_numpyarray = np.array([np.array(xi) for xi in X])\n",
    "        X = X_numpyarray\n",
    "\n",
    "    if input_num == 4:\n",
    "        for i in range(len(df)):\n",
    "            X_pair = []\n",
    "            X_pair.append(df.col1[i])\n",
    "            X_pair.append(df.col2[i])\n",
    "            X_pair.append(df.col1[i]**2) #X3 is X1**2\n",
    "            X_pair.append(df.col2[i]**2) #X4 is X1**2\n",
    "            X_pair.append(df.col1[i] * df.col2[i])  # X5 is X1 * X2\n",
    "            X.append(X_pair)\n",
    "        X_numpyarray = np.array([np.array(xi) for xi in X])\n",
    "        X = X_numpyarray\n",
    "\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function above will build the X feature attributes based on input_num. If input_num = 1 it will build an X3 and X4 pair, if input_num is 2 it will build X3 and X5 pair, as outlined below. \n",
    "\n",
    "1. {X3, X4}\n",
    "2. {X3, X5}\n",
    "3. {X3, X4, X5}\n",
    "4. {X1, X2, X3, X4, X5}\n",
    "0. default - original attribute in the dataset, X1 and X2\n",
    "\n",
    "The computations for these X values are as follows:\n",
    "\n",
    "Formula:\n",
    "\n",
    "Xsub3 = Xsub1^2\n",
    "\n",
    "Xsub4 = Xsub2^2\n",
    "\n",
    "Xsub5 = Xsub1 * Xsub2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we build the y attribute. This is the target class, the 3rd column in our train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_y(df):\n",
    "    y = []\n",
    "    for i in range(len(df)):\n",
    "        y.append(df.col3_class[i])\n",
    "    y_numpyarray = np.array([np.array(yi) for yi in y])\n",
    "    y = y_numpyarray\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Build a Neural Network Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our neural network classifier I used SKLearn's multi-layer perceptron classifier.\n",
    "\n",
    "MLPClassifier is a multilayer perceptron (MLP). A class of feedforward artificial neural network. A MLP consists of at least three layers of nodes: an input layer, a hidden layer and an output layer. Except for the input nodes, each node is a neuron that uses a nonlinear activation function [3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def mlp_classifier(hidden_layer_sizes, activation='relu', solver='lbfgs'):\n",
    "    clf = MLPClassifier(activation=activation, solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(hidden_layer_sizes), random_state=1)\n",
    "    # abov: hidden_layer_sizes, first digit is hidden units, second digit is hidden layer\n",
    "    clf = clf.fit(X_for_mlp, y_for_mlp)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to MLP Classifier, let's use Keras NN library.\n",
    "\n",
    "Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research[4]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# define the keras model\n",
    "def keras_classifier():\n",
    "    keras_model = Sequential()\n",
    "    keras_model.add(Dense(12, input_dim=input_dim, activation='relu'))\n",
    "    keras_model.add(Dense(8, activation='relu'))\n",
    "    keras_model.add(Dense(3, activation='sigmoid'))\n",
    "    #keras_model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile the keras model\n",
    "    keras_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    # fit the keras model on the dataset\n",
    "    #keras_model.fit(X_for_keras, y_for_keras, epochs=150, batch_size=10, verbose=0)\n",
    "    keras_model.fit(X_for_mlp, y_for_keras, epochs=150, batch_size=10, verbose=0)\n",
    "\n",
    "    # make class predictions with the model\n",
    "    keras_predictions = keras_model.predict_classes(X_for_mlp)\n",
    "    return keras_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can use the Keras function above for prediction, the target class value in our dataset must be encoded to a numeric value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_target_class():\n",
    "    pd_encoded_output = pd.get_dummies(df, columns=[\"col3_class\"])\n",
    "    encoded_output = pd_encoded_output.values[0:, 2:5] # the last encoded row, which is \"r\". 2 is \"b\", 3 is \"g\"\n",
    "    #y = pd_encoded_output.values[0:, 3] # index 4 is \"r\", index 3 is \"g\", index 2 is \"b\"\n",
    "    y_for_keras = encoded_output\n",
    "    return y_for_keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the encode_target_class function above I used the Pandas get_dummies() function[5].\n",
    "\n",
    "The get_dummies function is a Pandas libray for converting categorical values to numeric values.\n",
    "\n",
    "Let's try it out..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       ...,\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_for_keras = encode_target_class()\n",
    "y_for_keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the col3_class values have been converted from \"r\", \"g\", \"b\" to a numeric binary value, with each values in having it's own column in the encoded dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now use these NN classifiers to predict some values using our train dataset. (next section)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Find the Simplest Neural Network\n",
    "#### Use different activations, number of layers, number of neurons at each layer, compare their performance and find the simplest neural net. There could be couple of networks that are fairly close in terms of the performance choose anyone you think has the least complexity and explain your reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Use different activations, number or layers, number of neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run our classifiers given different activation, layers, and neuron. To do this, we'll have to set variables that we'll use as inputs to our classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_sizes = [5, 2] #original hidden_layer_sizes\n",
    "#hidden_layer_sizes = [10, 10, 10]\n",
    "\n",
    "# Activations: ‘identity’, ‘logistic’, ‘tanh’, ‘relu’\n",
    "mlp_activation = 'relu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hidden layers sizes are set to two (5, 2), with activation values as either identity, logistic, tanh, or relu.\n",
    "\n",
    "Let's also build our input features, X, as well as our target feature, y. We'll build X based on input_num, which determinses our set of transformed values for X. i.e. (calculated given the formula shown in the first section of this notebook)\n",
    "\n",
    "\n",
    "   1. {X3, X4}\n",
    "   2. {X3, X5}\n",
    "   3. {X3, X4, X5}\n",
    "   4. {X1, X2, X3, X4, X5}\n",
    "   0. default - original attribute in the dataset, X1 and X2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_num = 0\n",
    "X_for_mlp = build_X(df, input_num)\n",
    "y_for_mlp = build_y(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we haver X and y, let's now run our classifier to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['r', 'r', 'r', 'r', 'r', 'g', 'r', 'r', 'r', 'g', 'r', 'r', 'r',\n",
       "       'g', 'r', 'r', 'r', 'b', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r',\n",
       "       'r', 'r', 'r', 'g', 'r', 'r', 'g', 'r', 'r', 'r', 'r', 'r', 'r',\n",
       "       'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'b', 'r',\n",
       "       'r', 'r', 'r', 'r', 'r', 'g', 'r', 'r', 'r', 'b', 'r', 'r', 'r',\n",
       "       'r', 'r', 'r', 'r', 'r', 'r', 'b', 'r', 'r', 'b', 'r', 'r', 'b',\n",
       "       'g', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r',\n",
       "       'b', 'r', 'r', 'r', 'r', 'r', 'r', 'g', 'r', 'r', 'r', 'r', 'r',\n",
       "       'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'g',\n",
       "       'r', 'r', 'r', 'r', 'r', 'r', 'r', 'b', 'r', 'r', 'r', 'r', 'r',\n",
       "       'g', 'r', 'b', 'r', 'r', 'r', 'b', 'g', 'b', 'r', 'r', 'r', 'r',\n",
       "       'b', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'b', 'r', 'r', 'r',\n",
       "       'b', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r',\n",
       "       'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r',\n",
       "       'r', 'r', 'b', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r',\n",
       "       'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'g', 'b', 'b', 'r', 'r',\n",
       "       'r', 'r', 'r', 'r', 'b', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r',\n",
       "       'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'b',\n",
       "       'g', 'r', 'r', 'r', 'r', 'b', 'r', 'r', 'r', 'r', 'r', 'r', 'g',\n",
       "       'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r',\n",
       "       'r', 'r', 'r', 'g', 'r', 'b', 'r', 'g', 'r', 'b', 'b', 'r', 'r',\n",
       "       'r', 'r', 'r', 'b', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'g',\n",
       "       'r', 'g', 'r', 'g', 'b', 'r', 'r', 'r', 'r', 'r', 'r', 'b', 'r',\n",
       "       'r', 'r', 'r', 'r', 'r', 'b', 'r', 'g', 'r', 'r', 'g', 'r', 'r',\n",
       "       'r', 'r', 'r', 'r', 'r', 'b', 'g', 'r', 'r', 'r', 'r', 'b', 'r',\n",
       "       'r', 'r', 'r', 'r', 'r', 'r', 'r', 'b', 'b', 'r', 'r', 'r', 'r',\n",
       "       'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'g', 'r', 'r', 'r',\n",
       "       'b', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'g', 'r', 'r',\n",
       "       'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r',\n",
       "       'r', 'r', 'r', 'r', 'b', 'r', 'r', 'r', 'r', 'r', 'b', 'r', 'r',\n",
       "       'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r',\n",
       "       'r', 'r', 'r', 'r', 'b', 'r', 'r', 'g', 'g', 'r', 'r', 'r', 'r',\n",
       "       'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r',\n",
       "       'r', 'r', 'r', 'b', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'g',\n",
       "       'r', 'r', 'b', 'r', 'b', 'g', 'r', 'r', 'r', 'r', 'r', 'r', 'r',\n",
       "       'r', 'r', 'r', 'r', 'r', 'b', 'r', 'r', 'r', 'r', 'r', 'r', 'r',\n",
       "       'b', 'g', 'r', 'r', 'g', 'r', 'r', 'r', 'g', 'b', 'r', 'r', 'r',\n",
       "       'g', 'b', 'r', 'b', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r',\n",
       "       'r', 'r', 'g', 'r', 'r', 'b', 'r', 'g', 'r', 'r', 'r', 'r', 'r',\n",
       "       'r', 'r', 'r', 'r', 'r', 'g', 'r', 'r', 'r', 'r', 'r', 'r', 'r',\n",
       "       'r', 'r', 'g', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r',\n",
       "       'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r',\n",
       "       'b', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r',\n",
       "       'r', 'r', 'r', 'r', 'g', 'r', 'r', 'r', 'r', 'r', 'r', 'g', 'r',\n",
       "       'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'b', 'r', 'r', 'r',\n",
       "       'r', 'r', 'r', 'b', 'g', 'g', 'r', 'g', 'r', 'r', 'r', 'r', 'r',\n",
       "       'g', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r',\n",
       "       'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'b', 'g', 'r', 'r', 'r',\n",
       "       'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'b', 'r',\n",
       "       'r', 'g', 'g', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r',\n",
       "       'g', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'g', 'r', 'r', 'r',\n",
       "       'g', 'r', 'r', 'r', 'r', 'r', 'r', 'r', 'g', 'r', 'g', 'r', 'r',\n",
       "       'r', 'b', 'r', 'r', 'r', 'r', 'r', 'r', 'r'], dtype='<U1')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = mlp_classifier(hidden_layer_sizes, activation=mlp_activation)\n",
    "mlp_classifier_predictions = clf.predict(X_for_mlp)\n",
    "\n",
    "mlp_classifier_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also try probability predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.12493477e-22, 9.73897867e-21, 1.00000000e+00],\n",
       "       [7.49967236e-24, 3.43153223e-19, 1.00000000e+00],\n",
       "       [5.37903021e-36, 2.13934071e-08, 9.99999979e-01],\n",
       "       ...,\n",
       "       [3.80650711e-23, 8.09843448e-20, 1.00000000e+00],\n",
       "       [4.26843967e-38, 1.57507601e-06, 9.99998425e-01],\n",
       "       [1.13222809e-24, 1.84225743e-18, 1.00000000e+00]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_classifier_prob_predictions = clf.predict_proba(X_for_mlp)\n",
    "mlp_classifier_prob_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Keras Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's predictions using the same X and y values, as well as input_num value, with Keras.\n",
    "\n",
    "But first, let's setup some values required by our Keras function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input_num == 0 or input_num == 1 or input_num == 2:\n",
    "    input_dim = 2\n",
    "if input_num == 3:\n",
    "    input_dim = 3\n",
    "if input_num == 4:\n",
    "    input_dim = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_for_keras = encode_target_class()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the input_num value, we set the number of input dimensions for creating our Keras model. input_dim is the number of values in the array given X1 and X2 values, the scaled or transformed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0819 15:46:55.633548 10272 deprecation_wrapper.py:119] From C:\\Users\\ramon\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0819 15:46:55.689508 10272 deprecation_wrapper.py:119] From C:\\Users\\ramon\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0819 15:46:55.713859 10272 deprecation_wrapper.py:119] From C:\\Users\\ramon\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0819 15:46:55.787861 10272 deprecation_wrapper.py:119] From C:\\Users\\ramon\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0819 15:46:55.803481 10272 deprecation_wrapper.py:119] From C:\\Users\\ramon\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0819 15:46:55.819102 10272 deprecation.py:323] From C:\\Users\\ramon\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0819 15:46:55.988335 10272 deprecation_wrapper.py:119] From C:\\Users\\ramon\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 0, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 0, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 2, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 1, 2,\n",
       "       0, 2, 2, 2, 0, 1, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2,\n",
       "       2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 1, 0, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 1, 2, 2, 2, 2, 0, 2, 2,\n",
       "       2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1,\n",
       "       2, 0, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 1,\n",
       "       2, 1, 2, 1, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2,\n",
       "       2, 1, 2, 2, 2, 2, 2, 2, 2, 0, 1, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 0,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 0, 2, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2,\n",
       "       2, 2, 2, 2, 2, 2, 0, 1, 2, 2, 1, 2, 2, 2, 1, 0, 2, 2, 2, 1, 0, 2,\n",
       "       0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 0, 2, 1, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 1, 1, 2, 1, 2,\n",
       "       2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2,\n",
       "       2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1,\n",
       "       2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 0, 2, 2, 2, 2,\n",
       "       2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_predictions = keras_classifier()\n",
    "keras_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Compare performance of classifiers using different activations, layers, neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare performance, let's create a function for measuring accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_measure(mlp_or_keras, turn_off_printouts=\"no\"):\n",
    "    correct_ctr = 0\n",
    "    accuracy_pct = 0\n",
    "    if mlp_or_keras == \"keras\":\n",
    "        for i in range(len(X_for_mlp)):\n",
    "            #print(str(X_for_keras[i]) + \"=>\" + \" predicted: \" + str(keras_predictions[i]) + \", expected: \" + str(y_for_keras[i]))\n",
    "            if int(keras_predictions[i]) == 0 and int(y_for_keras[i][0]) == 1:\n",
    "                correct_ctr += 1\n",
    "                #print(str(X_for_keras[i]) + \"=>\" + \" predicted: \" + str(keras_predictions[i]) + \", expected: \" + str(\n",
    "                #    y_for_keras[i]))\n",
    "            if int(keras_predictions[i]) == 1 and int(y_for_keras[i][1]) == 1:\n",
    "                correct_ctr += 1\n",
    "                #print(str(X_for_keras[i]) + \"=>\" + \" predicted: \" + str(keras_predictions[i]) + \", expected: \" + str(\n",
    "                #    y_for_keras[i]))\n",
    "            if int(keras_predictions[i]) == 2 and int(y_for_keras[i][2]) == 1:\n",
    "                correct_ctr += 1\n",
    "                #print(str(X_for_keras[i]) + \"=>\" + \" predicted: \" + str(keras_predictions[i]) + \", expected: \" + str(\n",
    "                #    y_for_keras[i]))\n",
    "\n",
    "        if turn_off_printouts == \"no\":\n",
    "            print(\"Keras - accuracy percentage:\")\n",
    "            print(\"correct predictions: \" + str(correct_ctr))\n",
    "            print(\"Number of rows/instances:\" + str(len(X_for_mlp)))\n",
    "            print(correct_ctr/len(X_for_mlp))\n",
    "\n",
    "    if mlp_or_keras == \"mlp\":\n",
    "        for i in range(len(X_for_mlp)):\n",
    "            #print(str(X_for_mlp[i]) + \"=>\" + \" predicted: \" + str(mlp_classifier_predictions[i]) +\n",
    "                # \", expected: \" + str(y_for_mlp[i]))\n",
    "            if mlp_classifier_predictions[i] == y_for_mlp[i]:\n",
    "                correct_ctr += 1\n",
    "        \n",
    "        if turn_off_printouts == \"no\":\n",
    "            print(\"MLP - accuracy percentage:\")\n",
    "            print(\"correct_ctr: \" + str(correct_ctr))\n",
    "            print(\"len(X_for_mlp):\" + str(len(X_for_mlp)))\n",
    "            print(correct_ctr/len(X_for_mlp))\n",
    "    \n",
    "    accuracy_pct = correct_ctr/len(X_for_mlp)\n",
    "    return accuracy_pct\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can call the accuracy measure function above, let's create different activations, layers, and neuron values for our classifiers. This means we'll have to rebuild our X and y attributes as well.\n",
    "\n",
    "In the following performance test section, I created random values and inputs to test the outcome of each classification's accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Performance Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_sizes = [5, 2] \n",
    "\n",
    "# Activations: ‘identity’, ‘logistic’, ‘tanh’, ‘relu’\n",
    "mlp_activation = 'relu'\n",
    "\n",
    "input_num = 0\n",
    "X_for_mlp = build_X(df, input_num)\n",
    "y_for_mlp = build_y(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now rebuild our MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = mlp_classifier(hidden_layer_sizes, activation=mlp_activation)\n",
    "mlp_classifier_predictions = clf.predict(X_for_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP - accuracy percentage:\n",
      "correct_ctr: 681\n",
      "len(X_for_mlp):685\n",
      "0.9941605839416059\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9941605839416059"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_measure(\"mlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras - accuracy percentage:\n",
      "correct predictions: 680\n",
      "Number of rows/instances:685\n",
      "0.9927007299270073\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9927007299270073"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_measure(\"keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But classifiers are at 99% accuracy given the randomly chosen inputs.\n",
    "\n",
    "Let's try a few more tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Performance Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hidden_layer_sizes = [5, 2] #original hidden_layer_sizes\n",
    "hidden_layer_sizes = [10, 10, 10]\n",
    "\n",
    "# Activations: ‘identity’, ‘logistic’, ‘tanh’, ‘relu’\n",
    "mlp_activation = 'logistic'\n",
    "\n",
    "input_num = 1\n",
    "X_for_mlp = build_X(df, input_num)\n",
    "y_for_mlp = build_y(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = mlp_classifier(hidden_layer_sizes, activation=mlp_activation)\n",
    "mlp_classifier_predictions = clf.predict(X_for_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP - accuracy percentage:\n",
      "correct_ctr: 635\n",
      "len(X_for_mlp):685\n",
      "0.927007299270073\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.927007299270073"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_measure(\"mlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input_num == 0 or input_num == 1 or input_num == 2:\n",
    "    input_dim = 2\n",
    "if input_num == 3:\n",
    "    input_dim = 3\n",
    "if input_num == 4:\n",
    "    input_dim = 5\n",
    "\n",
    "y_for_keras = encode_target_class()\n",
    "keras_predictions = keras_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras - accuracy percentage:\n",
      "correct predictions: 654\n",
      "Number of rows/instances:685\n",
      "0.9547445255474453\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9547445255474453"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_measure(\"keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Performance Test 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_sizes = [6, 3, 2] #original hidden_layer_sizes\n",
    "#hidden_layer_sizes = [10, 10, 10]\n",
    "\n",
    "# Activations: ‘identity’, ‘logistic’, ‘tanh’, ‘relu’\n",
    "mlp_activation = 'logistic'\n",
    "\n",
    "input_num = 3\n",
    "X_for_mlp = build_X(df, input_num)\n",
    "y_for_mlp = build_y(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = mlp_classifier(hidden_layer_sizes, activation=mlp_activation)\n",
    "mlp_classifier_predictions = clf.predict(X_for_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP - accuracy percentage:\n",
      "correct_ctr: 635\n",
      "len(X_for_mlp):685\n",
      "0.927007299270073\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.927007299270073"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_measure(\"mlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input_num == 0 or input_num == 1 or input_num == 2:\n",
    "    input_dim = 2\n",
    "if input_num == 3:\n",
    "    input_dim = 3\n",
    "if input_num == 4:\n",
    "    input_dim = 5\n",
    "\n",
    "y_for_keras = encode_target_class()\n",
    "keras_predictions = keras_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras - accuracy percentage:\n",
      "correct predictions: 635\n",
      "Number of rows/instances:685\n",
      "0.927007299270073\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.927007299270073"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_measure(\"keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4 Performance Test 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_sizes = [2, 1, 1] #original hidden_layer_sizes\n",
    "#hidden_layer_sizes = [10, 10, 10]\n",
    "\n",
    "# Activations: ‘identity’, ‘logistic’, ‘tanh’, ‘relu’\n",
    "mlp_activation = 'relu'\n",
    "\n",
    "input_num = 0\n",
    "X_for_mlp = build_X(df, input_num)\n",
    "y_for_mlp = build_y(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = mlp_classifier(hidden_layer_sizes, activation=mlp_activation)\n",
    "mlp_classifier_predictions = clf.predict(X_for_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP - accuracy percentage:\n",
      "correct_ctr: 585\n",
      "len(X_for_mlp):685\n",
      "0.8540145985401459\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8540145985401459"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_measure(\"mlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "if input_num == 0 or input_num == 1 or input_num == 2:\n",
    "    input_dim = 2\n",
    "if input_num == 3:\n",
    "    input_dim = 3\n",
    "if input_num == 4:\n",
    "    input_dim = 5\n",
    "\n",
    "y_for_keras = encode_target_class()\n",
    "print(y_for_keras)\n",
    "keras_predictions = keras_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras - accuracy percentage:\n",
      "correct predictions: 681\n",
      "Number of rows/instances:685\n",
      "0.9941605839416059\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9941605839416059"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_measure(\"keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance tests above shows around 98% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Build New Attributes\n",
    "#### In this section you will create new attributes and you are going to use them instead to train the neural network. If we call the first attributes X1 and the second attributes X2, we can build new attributes.\n",
    "#### X3 = X2/1, X4 = X2/2, X5 = X1X2\n",
    "#### Find the simplest Neural network for the following set of inputs: (The data that you feed into the neural network.)\n",
    "#### 1. {X3;X4}<br>2. {X3;X5}<br>3. {X3;X4;X5}<br>4. {X1;X2;X3;X4;X5}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our <i>build_X()</i> and <i>build_y()</i> functions above already supports re-buildling of our X and y attributes. They have been demonstrated above in the performance accuracy section.\n",
    "\n",
    "In this section, we'll dynamically build these attributes by looping through different values, while also performing accuracy measure at runtime. This is so that we can identify the parameters that yields the highest accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Rebuild attributes using function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's rebuild new attributes for our function. The attributes are based on the values X1 and X2 computations provided in the homework material, and is shown in the first section of this paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebuild_attributes_based_on_params(hidden_layer_sizes, activation, input_num):\n",
    "    hidden_layer_sizes = hidden_layer_sizes\n",
    "    mlp_activation = activation\n",
    "    X_for_mlp = build_X(df, input_num)\n",
    "    y_for_mlp = build_y(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's randomly pick a few input values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_sizes = [16, 4] #original hidden_layer_sizes\n",
    "#hidden_layer_sizes = [10, 10, 10]\n",
    "\n",
    "# Activations: ‘identity’, ‘logistic’, ‘tanh’, ‘relu’\n",
    "mlp_activation = 'relu'\n",
    "input_num = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run our attribute build function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebuild_attributes_based_on_params(hidden_layer_sizes, mlp_activation, input_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the attribute have been rebuilt, we have to rerun them in our classifer. See below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = mlp_classifier(hidden_layer_sizes, activation=mlp_activation)\n",
    "mlp_classifier_predictions = clf.predict(X_for_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have an updated MLP Classifier prediction that is based on the rebuilt or scaled attributes. Let's use them below to find the best model for different input values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Find best model for different input values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Activation: relu\n",
      "0.8540145985401459\n",
      "0.9927007299270073\n",
      "1.0\n",
      "0.9985401459854014\n",
      "1.0\n",
      "0.997080291970803\n",
      "1.0\n",
      "0.9985401459854014\n",
      "0.9985401459854014\n",
      "1.0\n",
      "Avg: 0.983941605839416\n",
      "\n",
      "MLP Activation: logistic\n",
      "0.8540145985401459\n",
      "0.927007299270073\n",
      "0.9386861313868613\n",
      "0.9255474452554745\n",
      "0.9635036496350365\n",
      "0.9343065693430657\n",
      "0.9401459854014599\n",
      "0.9737226277372263\n",
      "0.9912408759124087\n",
      "0.9708029197080292\n",
      "Avg: 0.9418978102189781\n",
      "\n",
      "MLP Activation: tanh\n",
      "0.927007299270073\n",
      "0.927007299270073\n",
      "0.9313868613138686\n",
      "0.9532846715328467\n",
      "0.964963503649635\n",
      "0.981021897810219\n",
      "0.962043795620438\n",
      "0.9401459854014599\n",
      "0.981021897810219\n",
      "0.9635036496350365\n",
      "Avg: 0.953138686131387\n",
      "\n",
      "MLP Activation: identity\n",
      "0.8540145985401459\n",
      "0.8540145985401459\n",
      "0.8540145985401459\n",
      "0.8540145985401459\n",
      "0.8540145985401459\n",
      "0.8540145985401459\n",
      "0.8540145985401459\n",
      "0.8540145985401459\n",
      "0.8540145985401459\n",
      "0.8540145985401459\n",
      "Avg: 0.8540145985401459\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#hidden_layer_sizes = [10, 10, 10]\n",
    "\n",
    "# Activations: ‘identity’, ‘logistic’, ‘tanh’, ‘relu’\n",
    "mlp_activation = ['relu', 'logistic', 'tanh', 'identity']\n",
    "input_num = 0\n",
    "\n",
    "for i in range(len(mlp_activation)): # Activation loop\n",
    "    print(\"MLP Activation: \" + mlp_activation[i])\n",
    "    index_size = 10\n",
    "    acc_pct_sum = []\n",
    "    for j in range(index_size):\n",
    "        hidden_layer_sizes = [j+3, j+2, j+1]\n",
    "        rebuild_attributes_based_on_params(hidden_layer_sizes, mlp_activation[i], input_num)\n",
    "        clf = mlp_classifier(hidden_layer_sizes, activation=mlp_activation[i])\n",
    "        mlp_classifier_predictions = clf.predict(X_for_mlp)\n",
    "        acc_pct = accuracy_measure(\"mlp\", turn_off_printouts=\"yes\")\n",
    "        print(acc_pct)\n",
    "        acc_pct_sum.append(acc_pct)\n",
    "    print(\"Avg: \" + str(sum(acc_pct_sum)/index_size))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the outcome of the sample test above, we can see that the best classifier, given a set of attributes, is with activation 'relu', and with three neuron layers, <i>j+3, j+2, j+1</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REFERENCES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] https://en.wikipedia.org/wiki/Neural_network\n",
    "\n",
    "[2] https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "\n",
    "[3] https://en.wikipedia.org/wiki/Multilayer_perceptron\n",
    "\n",
    "[4] https://keras.io/\n",
    "\n",
    "[5] https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html\n",
    "\n",
    "[6] #https://www.python-course.eu/neural_networks_with_scikit.php\n",
    "\n",
    "[7] https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
